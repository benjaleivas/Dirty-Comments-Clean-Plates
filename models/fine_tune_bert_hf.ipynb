{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with hugging face wrapper for sanity check\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training hyperparameters\n",
    "MAX_TOKENS = 512\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 8\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-05\n",
    "\n",
    "# change to true to run per review\n",
    "EXPANDED = False\n",
    "\n",
    "model_checkpoint = \"distilbert/distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.read_csv('../data/split/train.csv', usecols=['Overall Compliance', 'reviews'])\n",
    "test_df = pd.read_csv('../data/split/test.csv', usecols=['Overall Compliance', 'reviews'])\n",
    "val_df = pd.read_csv('../data/split/val.csv', usecols=['Overall Compliance', 'reviews'])\n",
    "review_df['reviews'] = review_df['reviews'].apply(literal_eval)\n",
    "test_df['reviews'] = test_df['reviews'].apply(literal_eval)\n",
    "val_df['reviews'] = val_df['reviews'].apply(literal_eval)\n",
    "\n",
    "# test classifying at reivew level then resturant level\n",
    "if EXPANDED:\n",
    "    review_df = review_df.explode('reviews')\n",
    "    review_df = review_df.reset_index().drop(columns=['index'])\n",
    "\n",
    "    test_df = test_df.explode('reviews')\n",
    "    test_df = test_df.reset_index().drop(columns=['index'])\n",
    "    \n",
    "    val_df = val_df.explode('reviews')\n",
    "    val_df = val_df.reset_index().drop(columns=['index'])\n",
    "\n",
    "train_ds = Dataset.from_pandas(review_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "\n",
    "review_dataset = DatasetDict()\n",
    "\n",
    "review_dataset['train'] = train_ds\n",
    "review_dataset['test'] = test_ds\n",
    "review_dataset['val'] = val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_reviews(reviews):\n",
    "    cleaned = []\n",
    "    for review in reviews:\n",
    "        review = review.replace('\\n', ' ')\n",
    "        cleaned.append(re.sub(r\"[^a-zA-Z0-9]\", ' ', review).strip()) #may need to find a better way to do so\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "def extract_text_features(data):\n",
    "    output = {}\n",
    "    reviews = clean_reviews(data['reviews'])\n",
    "    output['text'] = \" \".join(reviews)\n",
    "    output['label'] = 0 if data['Overall Compliance'] == 'Yes' else 1\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def extract_expanded_text_features(data):\n",
    "    output = {}\n",
    "    review = data['reviews']\n",
    "    review = review.replace('\\n', ' ')\n",
    "    output['text'] = re.sub(r\"[^a-zA-Z0-9]\", ' ', review).strip()\n",
    "    output['label'] = 0 if data['Overall Compliance'] == 'Yes' else 1\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1754/1754 [00:00<00:00, 3867.16 examples/s]\n",
      "Map: 100%|██████████| 195/195 [00:00<00:00, 3836.24 examples/s]\n",
      "Map: 100%|██████████| 216/216 [00:00<00:00, 3969.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "extract_function = extract_text_features\n",
    "\n",
    "if EXPANDED:\n",
    "    extract_function = extract_expanded_text_features\n",
    "\n",
    "review_dataset = review_dataset.map(extract_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Overall Compliance': 'No',\n",
       " 'reviews': [\"(3.5) ~ good overall food service.\\n\\nMENU:\\n* Plain Pizza = (3.75) Good.  It's similar in style & texture to the original pan pizzas that you'd get at Pizza Hut.\\n* French Fries = (3.25) pretty good\",\n",
       "  'Just had pizza from here for 1st time. It was delicious. Light crust lots of cheese. Definitely will reorder.',\n",
       "  \"Review for cheesesteak only. I've been searching the Main Line for good cheesesteaks and this is the place I'd go when in the Devon/Berwyn area (see my other reviews for where'd I'd go in Wayne). This place makes a proper cheesesteak.\",\n",
       "  'One of my favorite spots!! Food is amazing, staff is super friendly and I love choosing this place to go eat.',\n",
       "  'Great greek-style pizza that has been serving the area for years.  Still family owned and they care about their quality.  I grew up on greek style pizza and I need my fix from time to time.  This is the place I go to for just that.  Beyond their pizza, their food is of great quality and really tasty.  They always have a smile on their face when you go in.',\n",
       "  \"Anchovie & Soinach Pizza: Extremely Delicious , Crust had alot of flavor and was Very light.\\nPepperoni, Onion, & Ricotta: \\nMouthwatering, Flavorful, Delightful\\nEmployees are Great. \\nNick: Great, Nice, and Friendly.\\nRich: *Master of his Craft* Amazing Pizza maker\\nWill DEFINETLY be coming back, I'm from CA but worth the trip.\",\n",
       "  'Deeeeeliscious! Consistently great cheesesteak sandwiches! Greek pizza is yummy. two thumbs all the way up!',\n",
       "  \"First place that I ever even heard of a stromboli while going to a school next door for rowdy kids named Devereux. The stromboli was amazing and every 5 or so years I go to King Of Prussia Mall and always stop in to Berwyn Pizza. The pizza is amazing to me. The crust is a medium thick and I normally prefer thin but this crust has a very unique texture as well as flavor which in todays pizza world seems to be an afterthought.  The sauce is also very unique but even better is whatever cheese combination that they use. I really love plain ole cheese pizza so I can't speak for the toppings but I am sure they are of equal quality. The stromboli that I always get is the Berwyn Special which is off the charts! I am a creature of habit so I stick to what I have tried and loved. Last time I was there was in 2007 but I am told that those items that I just reviewed are still being made the same!\"],\n",
       " 'text': '3 5    good overall food service   MENU    Plain Pizza    3 75  Good   It s similar in style   texture to the original pan pizzas that you d get at Pizza Hut    French Fries    3 25  pretty good Just had pizza from here for 1st time  It was delicious  Light crust lots of cheese  Definitely will reorder Review for cheesesteak only  I ve been searching the Main Line for good cheesesteaks and this is the place I d go when in the Devon Berwyn area  see my other reviews for where d I d go in Wayne   This place makes a proper cheesesteak One of my favorite spots   Food is amazing  staff is super friendly and I love choosing this place to go eat Great greek style pizza that has been serving the area for years   Still family owned and they care about their quality   I grew up on greek style pizza and I need my fix from time to time   This is the place I go to for just that   Beyond their pizza  their food is of great quality and really tasty   They always have a smile on their face when you go in Anchovie   Soinach Pizza  Extremely Delicious   Crust had alot of flavor and was Very light  Pepperoni  Onion    Ricotta   Mouthwatering  Flavorful  Delightful Employees are Great   Nick  Great  Nice  and Friendly  Rich   Master of his Craft  Amazing Pizza maker Will DEFINETLY be coming back  I m from CA but worth the trip Deeeeeliscious  Consistently great cheesesteak sandwiches  Greek pizza is yummy  two thumbs all the way up First place that I ever even heard of a stromboli while going to a school next door for rowdy kids named Devereux  The stromboli was amazing and every 5 or so years I go to King Of Prussia Mall and always stop in to Berwyn Pizza  The pizza is amazing to me  The crust is a medium thick and I normally prefer thin but this crust has a very unique texture as well as flavor which in todays pizza world seems to be an afterthought   The sauce is also very unique but even better is whatever cheese combination that they use  I really love plain ole cheese pizza so I can t speak for the toppings but I am sure they are of equal quality  The stromboli that I always get is the Berwyn Special which is off the charts  I am a creature of habit so I stick to what I have tried and loved  Last time I was there was in 2007 but I am told that those items that I just reviewed are still being made the same',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_dataset['train'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], max_length=MAX_TOKENS, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1754/1754 [00:04<00:00, 361.23 examples/s]\n",
      "Map: 100%|██████████| 195/195 [00:00<00:00, 344.23 examples/s]\n",
      "Map: 100%|██████████| 216/216 [00:00<00:00, 310.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "token_dataset = review_dataset.map(tokenize, remove_columns=['Overall Compliance', 'reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '3 5    good overall food service   MENU    Plain Pizza    3 75  Good   It s similar in style   texture to the original pan pizzas that you d get at Pizza Hut    French Fries    3 25  pretty good Just had pizza from here for 1st time  It was delicious  Light crust lots of cheese  Definitely will reorder Review for cheesesteak only  I ve been searching the Main Line for good cheesesteaks and this is the place I d go when in the Devon Berwyn area  see my other reviews for where d I d go in Wayne   This place makes a proper cheesesteak One of my favorite spots   Food is amazing  staff is super friendly and I love choosing this place to go eat Great greek style pizza that has been serving the area for years   Still family owned and they care about their quality   I grew up on greek style pizza and I need my fix from time to time   This is the place I go to for just that   Beyond their pizza  their food is of great quality and really tasty   They always have a smile on their face when you go in Anchovie   Soinach Pizza  Extremely Delicious   Crust had alot of flavor and was Very light  Pepperoni  Onion    Ricotta   Mouthwatering  Flavorful  Delightful Employees are Great   Nick  Great  Nice  and Friendly  Rich   Master of his Craft  Amazing Pizza maker Will DEFINETLY be coming back  I m from CA but worth the trip Deeeeeliscious  Consistently great cheesesteak sandwiches  Greek pizza is yummy  two thumbs all the way up First place that I ever even heard of a stromboli while going to a school next door for rowdy kids named Devereux  The stromboli was amazing and every 5 or so years I go to King Of Prussia Mall and always stop in to Berwyn Pizza  The pizza is amazing to me  The crust is a medium thick and I normally prefer thin but this crust has a very unique texture as well as flavor which in todays pizza world seems to be an afterthought   The sauce is also very unique but even better is whatever cheese combination that they use  I really love plain ole cheese pizza so I can t speak for the toppings but I am sure they are of equal quality  The stromboli that I always get is the Berwyn Special which is off the charts  I am a creature of habit so I stick to what I have tried and loved  Last time I was there was in 2007 but I am told that those items that I just reviewed are still being made the same',\n",
       " 'label': 1,\n",
       " 'input_ids': [101,\n",
       "  1017,\n",
       "  1019,\n",
       "  2204,\n",
       "  3452,\n",
       "  2833,\n",
       "  2326,\n",
       "  12183,\n",
       "  5810,\n",
       "  10733,\n",
       "  1017,\n",
       "  4293,\n",
       "  2204,\n",
       "  2009,\n",
       "  1055,\n",
       "  2714,\n",
       "  1999,\n",
       "  2806,\n",
       "  14902,\n",
       "  2000,\n",
       "  1996,\n",
       "  2434,\n",
       "  6090,\n",
       "  10733,\n",
       "  2015,\n",
       "  2008,\n",
       "  2017,\n",
       "  1040,\n",
       "  2131,\n",
       "  2012,\n",
       "  10733,\n",
       "  12570,\n",
       "  2413,\n",
       "  22201,\n",
       "  1017,\n",
       "  2423,\n",
       "  3492,\n",
       "  2204,\n",
       "  2074,\n",
       "  2018,\n",
       "  10733,\n",
       "  2013,\n",
       "  2182,\n",
       "  2005,\n",
       "  3083,\n",
       "  2051,\n",
       "  2009,\n",
       "  2001,\n",
       "  12090,\n",
       "  2422,\n",
       "  19116,\n",
       "  7167,\n",
       "  1997,\n",
       "  8808,\n",
       "  5791,\n",
       "  2097,\n",
       "  2128,\n",
       "  8551,\n",
       "  2121,\n",
       "  3319,\n",
       "  2005,\n",
       "  8808,\n",
       "  13473,\n",
       "  4817,\n",
       "  2069,\n",
       "  1045,\n",
       "  2310,\n",
       "  2042,\n",
       "  6575,\n",
       "  1996,\n",
       "  2364,\n",
       "  2240,\n",
       "  2005,\n",
       "  2204,\n",
       "  8808,\n",
       "  13473,\n",
       "  29243,\n",
       "  1998,\n",
       "  2023,\n",
       "  2003,\n",
       "  1996,\n",
       "  2173,\n",
       "  1045,\n",
       "  1040,\n",
       "  2175,\n",
       "  2043,\n",
       "  1999,\n",
       "  1996,\n",
       "  7614,\n",
       "  2022,\n",
       "  2099,\n",
       "  11761,\n",
       "  2181,\n",
       "  2156,\n",
       "  2026,\n",
       "  2060,\n",
       "  4391,\n",
       "  2005,\n",
       "  2073,\n",
       "  1040,\n",
       "  1045,\n",
       "  1040,\n",
       "  2175,\n",
       "  1999,\n",
       "  6159,\n",
       "  2023,\n",
       "  2173,\n",
       "  3084,\n",
       "  1037,\n",
       "  5372,\n",
       "  8808,\n",
       "  13473,\n",
       "  4817,\n",
       "  2028,\n",
       "  1997,\n",
       "  2026,\n",
       "  5440,\n",
       "  7516,\n",
       "  2833,\n",
       "  2003,\n",
       "  6429,\n",
       "  3095,\n",
       "  2003,\n",
       "  3565,\n",
       "  5379,\n",
       "  1998,\n",
       "  1045,\n",
       "  2293,\n",
       "  10549,\n",
       "  2023,\n",
       "  2173,\n",
       "  2000,\n",
       "  2175,\n",
       "  4521,\n",
       "  2307,\n",
       "  3306,\n",
       "  2806,\n",
       "  10733,\n",
       "  2008,\n",
       "  2038,\n",
       "  2042,\n",
       "  3529,\n",
       "  1996,\n",
       "  2181,\n",
       "  2005,\n",
       "  2086,\n",
       "  2145,\n",
       "  2155,\n",
       "  3079,\n",
       "  1998,\n",
       "  2027,\n",
       "  2729,\n",
       "  2055,\n",
       "  2037,\n",
       "  3737,\n",
       "  1045,\n",
       "  3473,\n",
       "  2039,\n",
       "  2006,\n",
       "  3306,\n",
       "  2806,\n",
       "  10733,\n",
       "  1998,\n",
       "  1045,\n",
       "  2342,\n",
       "  2026,\n",
       "  8081,\n",
       "  2013,\n",
       "  2051,\n",
       "  2000,\n",
       "  2051,\n",
       "  2023,\n",
       "  2003,\n",
       "  1996,\n",
       "  2173,\n",
       "  1045,\n",
       "  2175,\n",
       "  2000,\n",
       "  2005,\n",
       "  2074,\n",
       "  2008,\n",
       "  3458,\n",
       "  2037,\n",
       "  10733,\n",
       "  2037,\n",
       "  2833,\n",
       "  2003,\n",
       "  1997,\n",
       "  2307,\n",
       "  3737,\n",
       "  1998,\n",
       "  2428,\n",
       "  11937,\n",
       "  21756,\n",
       "  2027,\n",
       "  2467,\n",
       "  2031,\n",
       "  1037,\n",
       "  2868,\n",
       "  2006,\n",
       "  2037,\n",
       "  2227,\n",
       "  2043,\n",
       "  2017,\n",
       "  2175,\n",
       "  1999,\n",
       "  2019,\n",
       "  9905,\n",
       "  13469,\n",
       "  2061,\n",
       "  3981,\n",
       "  2818,\n",
       "  10733,\n",
       "  5186,\n",
       "  12090,\n",
       "  19116,\n",
       "  2018,\n",
       "  2632,\n",
       "  4140,\n",
       "  1997,\n",
       "  14894,\n",
       "  1998,\n",
       "  2001,\n",
       "  2200,\n",
       "  2422,\n",
       "  11565,\n",
       "  10698,\n",
       "  20949,\n",
       "  7043,\n",
       "  5946,\n",
       "  2677,\n",
       "  5880,\n",
       "  2075,\n",
       "  14894,\n",
       "  3993,\n",
       "  26380,\n",
       "  5126,\n",
       "  2024,\n",
       "  2307,\n",
       "  4172,\n",
       "  2307,\n",
       "  3835,\n",
       "  1998,\n",
       "  5379,\n",
       "  4138,\n",
       "  3040,\n",
       "  1997,\n",
       "  2010,\n",
       "  7477,\n",
       "  6429,\n",
       "  10733,\n",
       "  9338,\n",
       "  2097,\n",
       "  9375,\n",
       "  14626,\n",
       "  2022,\n",
       "  2746,\n",
       "  2067,\n",
       "  1045,\n",
       "  1049,\n",
       "  2013,\n",
       "  6187,\n",
       "  2021,\n",
       "  4276,\n",
       "  1996,\n",
       "  4440,\n",
       "  9266,\n",
       "  4402,\n",
       "  29282,\n",
       "  18436,\n",
       "  10862,\n",
       "  2307,\n",
       "  8808,\n",
       "  13473,\n",
       "  4817,\n",
       "  22094,\n",
       "  3306,\n",
       "  10733,\n",
       "  2003,\n",
       "  9805,\n",
       "  18879,\n",
       "  2048,\n",
       "  16784,\n",
       "  2035,\n",
       "  1996,\n",
       "  2126,\n",
       "  2039,\n",
       "  2034,\n",
       "  2173,\n",
       "  2008,\n",
       "  1045,\n",
       "  2412,\n",
       "  2130,\n",
       "  2657,\n",
       "  1997,\n",
       "  1037,\n",
       "  2358,\n",
       "  21716,\n",
       "  14956,\n",
       "  2072,\n",
       "  2096,\n",
       "  2183,\n",
       "  2000,\n",
       "  1037,\n",
       "  2082,\n",
       "  2279,\n",
       "  2341,\n",
       "  2005,\n",
       "  5216,\n",
       "  5149,\n",
       "  4268,\n",
       "  2315,\n",
       "  16475,\n",
       "  7869,\n",
       "  5602,\n",
       "  1996,\n",
       "  2358,\n",
       "  21716,\n",
       "  14956,\n",
       "  2072,\n",
       "  2001,\n",
       "  6429,\n",
       "  1998,\n",
       "  2296,\n",
       "  1019,\n",
       "  2030,\n",
       "  2061,\n",
       "  2086,\n",
       "  1045,\n",
       "  2175,\n",
       "  2000,\n",
       "  2332,\n",
       "  1997,\n",
       "  10724,\n",
       "  6670,\n",
       "  1998,\n",
       "  2467,\n",
       "  2644,\n",
       "  1999,\n",
       "  2000,\n",
       "  2022,\n",
       "  2099,\n",
       "  11761,\n",
       "  10733,\n",
       "  1996,\n",
       "  10733,\n",
       "  2003,\n",
       "  6429,\n",
       "  2000,\n",
       "  2033,\n",
       "  1996,\n",
       "  19116,\n",
       "  2003,\n",
       "  1037,\n",
       "  5396,\n",
       "  4317,\n",
       "  1998,\n",
       "  1045,\n",
       "  5373,\n",
       "  9544,\n",
       "  4857,\n",
       "  2021,\n",
       "  2023,\n",
       "  19116,\n",
       "  2038,\n",
       "  1037,\n",
       "  2200,\n",
       "  4310,\n",
       "  14902,\n",
       "  2004,\n",
       "  2092,\n",
       "  2004,\n",
       "  14894,\n",
       "  2029,\n",
       "  1999,\n",
       "  2651,\n",
       "  2015,\n",
       "  10733,\n",
       "  2088,\n",
       "  3849,\n",
       "  2000,\n",
       "  2022,\n",
       "  2019,\n",
       "  2044,\n",
       "  2705,\n",
       "  10593,\n",
       "  2102,\n",
       "  1996,\n",
       "  12901,\n",
       "  2003,\n",
       "  2036,\n",
       "  2200,\n",
       "  4310,\n",
       "  2021,\n",
       "  2130,\n",
       "  2488,\n",
       "  2003,\n",
       "  3649,\n",
       "  8808,\n",
       "  5257,\n",
       "  2008,\n",
       "  2027,\n",
       "  2224,\n",
       "  1045,\n",
       "  2428,\n",
       "  2293,\n",
       "  5810,\n",
       "  15589,\n",
       "  8808,\n",
       "  10733,\n",
       "  2061,\n",
       "  1045,\n",
       "  2064,\n",
       "  1056,\n",
       "  3713,\n",
       "  2005,\n",
       "  1996,\n",
       "  22286,\n",
       "  2015,\n",
       "  2021,\n",
       "  1045,\n",
       "  2572,\n",
       "  2469,\n",
       "  2027,\n",
       "  2024,\n",
       "  1997,\n",
       "  5020,\n",
       "  3737,\n",
       "  1996,\n",
       "  2358,\n",
       "  21716,\n",
       "  14956,\n",
       "  2072,\n",
       "  2008,\n",
       "  1045,\n",
       "  2467,\n",
       "  2131,\n",
       "  2003,\n",
       "  1996,\n",
       "  2022,\n",
       "  2099,\n",
       "  11761,\n",
       "  2569,\n",
       "  2029,\n",
       "  2003,\n",
       "  2125,\n",
       "  1996,\n",
       "  6093,\n",
       "  1045,\n",
       "  2572,\n",
       "  1037,\n",
       "  6492,\n",
       "  1997,\n",
       "  10427,\n",
       "  2061,\n",
       "  1045,\n",
       "  6293,\n",
       "  2000,\n",
       "  2054,\n",
       "  1045,\n",
       "  2031,\n",
       "  2699,\n",
       "  1998,\n",
       "  3866,\n",
       "  2197,\n",
       "  2051,\n",
       "  1045,\n",
       "  2001,\n",
       "  2045,\n",
       "  2001,\n",
       "  1999,\n",
       "  2289,\n",
       "  2021,\n",
       "  1045,\n",
       "  2572,\n",
       "  2409,\n",
       "  2008,\n",
       "  2216,\n",
       "  5167,\n",
       "  2008,\n",
       "  1045,\n",
       "  2074,\n",
       "  8182,\n",
       "  2024,\n",
       "  2145,\n",
       "  2108,\n",
       "  2081,\n",
       "  1996,\n",
       "  2168,\n",
       "  102],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_dataset['train'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 7.36k/7.36k [00:00<00:00, 9.50MB/s]\n",
      "Downloading builder script: 100%|██████████| 6.77k/6.77k [00:00<00:00, 14.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "recall_metric = evaluate.load('recall')\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    return {'accuracy': accuracy, 'recall': recall, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 1.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_metric.compute(predictions=[0,1], references=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"Pass\", 1: \"Fail\"}\n",
    "label2id = {\"Pass\": 0, \"Fail\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='440' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [440/440 05:06, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.472015</td>\n",
       "      <td>{'accuracy': 0.8205128205128205}</td>\n",
       "      <td>{'recall': 0.0}</td>\n",
       "      <td>{'f1': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.465191</td>\n",
       "      <td>{'accuracy': 0.8205128205128205}</td>\n",
       "      <td>{'recall': 0.0}</td>\n",
       "      <td>{'f1': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.458886</td>\n",
       "      <td>{'accuracy': 0.8205128205128205}</td>\n",
       "      <td>{'recall': 0.0}</td>\n",
       "      <td>{'f1': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.458501</td>\n",
       "      <td>{'accuracy': 0.8205128205128205}</td>\n",
       "      <td>{'recall': 0.0}</td>\n",
       "      <td>{'f1': 0.0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        4.0\n",
      "  total_flos               =   865563GF\n",
      "  train_loss               =     0.5201\n",
      "  train_runtime            = 0:05:07.54\n",
      "  train_samples_per_second =     22.813\n",
      "  train_steps_per_second   =      1.431\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"base_bert_model\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=VALID_BATCH_SIZE,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=token_dataset[\"train\"],\n",
    "    eval_dataset=token_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "trainer.log_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_metrics(\"train\", train_result.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "metrics = trainer.evaluate(token_dataset['val'])\n",
    "\n",
    "file_path = \"base_bert_model/val_results.json\"\n",
    "\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(metrics, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r base_bert.zip base_bert_model"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "85ea4b3f8f8bbee1e23da4c155a93a1e9f6833b5217ca791a452f768bdc9cb7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
