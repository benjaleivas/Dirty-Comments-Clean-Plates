{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZhazRa5y9ba"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.functional import log_softmax\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig\n",
        "from ast import literal_eval\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dmrQ8Pzy9bc"
      },
      "outputs": [],
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#training hyperparameters\n",
        "MAX_TOKENS = 512\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 8\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "\n",
        "# change to true to run per review\n",
        "EXPANDED = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "O1Zin1X0y9bc",
        "outputId": "1071bfd3-caf0-4df5-d5d2-c17130906256"
      },
      "outputs": [],
      "source": [
        "review_df = pd.read_csv('../data/split/train.csv')\n",
        "test_df = pd.read_csv('../data/split/test.csv')\n",
        "review_df['reviews'] = review_df['reviews'].apply(literal_eval)\n",
        "test_df['reviews'] = test_df['reviews'].apply(literal_eval)\n",
        "\n",
        "# test classifying at reivew level then resturant level\n",
        "if EXPANDED:\n",
        "    review_df = review_df.explode('reviews')\n",
        "    review_df = review_df.reset_index().drop(columns=['index'])\n",
        "\n",
        "    test_df = test_df.explode('reviews')\n",
        "    test_df = test_df.reset_index().drop(columns=['index'])\n",
        "    \n",
        "review_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8WRImmby9bd"
      },
      "outputs": [],
      "source": [
        "#mode info\n",
        "model_checkpoint = \"distilbert/distilbert-base-uncased\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#categorial features\n",
        "FEATURES = [\n",
        "    \"stars\",\n",
        "    \"review_count\",\n",
        "    \"is_open\",\n",
        "    \"n_reviews\",\n",
        "    \"avg_rating\",\n",
        "    \"IR_regular\",\n",
        "    \"IR_follow_up\",\n",
        "    \"IR_other\",\n",
        "    \"Chester\",\n",
        "    \"Bucks\",\n",
        "    \"Philadelphia\",\n",
        "    \"Delaware\",\n",
        "    \"Montgomery\",\n",
        "    \"Berks\",\n",
        "    'Nightlife',\n",
        "    'Bars',\n",
        "    'Pizza',\n",
        "    'Italian',\n",
        "    'Sandwiches',\n",
        "    'Breakfast & Brunch',\n",
        "    'Cafes',\n",
        "    'Burgers',\n",
        "    'Delis',\n",
        "    'Caterers',\n",
        "    'Mexican',\n",
        "    'Desserts',\n",
        "    'Salad',\n",
        "    'Sports Bars',\n",
        "    'Pubs',\n",
        "    'Chicken Wings',\n",
        "    'Seafood',\n",
        "    'Beer',\n",
        "    'Wine & Spirits',\n",
        "    'Juice Bars & Smoothies',\n",
        "    'Mediterranean',\n",
        "    'Gastropubs',\n",
        "    'Diners',\n",
        "    'Steakhouses',\n",
        "    'Breweries',\n",
        "    'Donuts',\n",
        "    'Barbeque',\n",
        "    'Cheesesteaks',\n",
        "    'Middle Eastern',\n",
        "    'Wineries',\n",
        "    'Indian',\n",
        "    'Halal',\n",
        "    'Vegan',\n",
        "    'Vegetarian',\n",
        "    'Beer Bar',\n",
        "    'Soup',\n",
        "    'Sushi Bars'\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wRSmT_4y9be"
      },
      "outputs": [],
      "source": [
        "class ReviewData(Dataset):\n",
        "\n",
        "    def __init__(self, df: pd.DataFrame, tokenizer : DistilBertTokenizer, max_tokens: int, expanded: bool = False, features: list = []):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.df = df\n",
        "        self.max_tokens = max_tokens\n",
        "        self.expanded = expanded\n",
        "        self.review_text = self.clean_text(self.df)\n",
        "        self.target_cat = self.df['Overall Compliance']\n",
        "        self.features = features\n",
        "\n",
        "\n",
        "    def clean_text(self, df: pd.DataFrame) -> pd.Series:\n",
        "\n",
        "        def clean_reviews(reviews):\n",
        "            cleaned = []\n",
        "            for review in reviews:\n",
        "                review = review.replace('\\n', ' ')\n",
        "                cleaned.append(re.sub(r\"[^a-zA-Z0-9]\", ' ', review).strip()) #may need to find a better way to do so\n",
        "            return cleaned\n",
        "\n",
        "        if self.expanded:\n",
        "            df['reviews'] = df['reviews'].str.strip()\n",
        "            df['reviews'] = df['reviews'].str.replace('\\n', ' ')\n",
        "            df['reviews'] = df['reviews'].str.replace(r\"[^a-zA-Z0-9]\", ' ', regex=True)\n",
        "            return df['reviews']\n",
        "        return df['reviews'].apply(clean_reviews)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.review_text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        review_text = str(self.review_text[index])\n",
        "        target_cat = self.target_cat[index]\n",
        "\n",
        "        cat_features = []\n",
        "        if self.features:\n",
        "            cat_features = self.df[self.features].iloc[index].values\n",
        "            \n",
        "        if not self.expanded:\n",
        "            # combine all reviews into one string\n",
        "            review_text = \" \".join(self.review_text[index])\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            review_text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_tokens,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        # [0, 1] = pass, [1, 0] = fail\n",
        "        target = []\n",
        "        if target_cat == 'No':\n",
        "            target = [1, 0]\n",
        "        else:\n",
        "            target = [0, 1]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'features': torch.tensor(cat_features, dtype=torch.long),\n",
        "            'targets': torch.tensor(target, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57cJGhghy9be"
      },
      "outputs": [],
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, do_lower_case=True)\n",
        "train_data = ReviewData(review_df, tokenizer, max_tokens=MAX_TOKENS, expanded=EXPANDED, features=FEATURES)\n",
        "test_data = ReviewData(test_df, tokenizer, max_tokens=MAX_TOKENS, expanded=EXPANDED, features=FEATURES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZskTDOUy9bf"
      },
      "outputs": [],
      "source": [
        "#load dataloaders\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(train_data, **train_params)\n",
        "testing_loader = DataLoader(test_data, **test_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfVDSM_ey9bi"
      },
      "outputs": [],
      "source": [
        "# BERT-based model\n",
        "\n",
        "class BERTAndErnie(torch.nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super(BERTAndErnie, self).__init__()\n",
        "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.l2 = torch.nn.Dropout(0.15)\n",
        "        self.l3 = torch.nn.Linear(768, 2)\n",
        "\n",
        "        #play with different activation functions\n",
        "        self.l4 = torch.nn.Softmax(dim=0)\n",
        "        # feature embeddings + \n",
        "        self.l5 = torch.nn.Linear(num_features + 2, 2)\n",
        "\n",
        "    def forward(self, ids, mask, features):\n",
        "        out1 = self.l1(ids, attention_mask=mask, return_dict=False)\n",
        "        # pull out tensor and reshape\n",
        "        hidden_layer = out1[0]\n",
        "   \n",
        "        #regularize with dropout\n",
        "        hidden_layer = self.l2(hidden_layer)\n",
        "   \n",
        "        # reshape to 16 x 768\n",
        "        bert_out = hidden_layer[:, 0]\n",
        "        print('step 1')\n",
        "        #drop to 16 X 2\n",
        "        \n",
        "        bert_out = self.l3(bert_out)\n",
        "        print('step 2')\n",
        "\n",
        "        # concate with BERT embeddings and apply non-linear\n",
        "        combined = self.l4(torch.cat((bert_out, features), dim=1)) \n",
        "\n",
        "        #collapse to label\n",
        "        out = self.l5(combined)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7qbVS02y9bi"
      },
      "outputs": [],
      "source": [
        "# check with Claire to standardize\n",
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qpc-ZXPiy9bj"
      },
      "outputs": [],
      "source": [
        "# initialize model and optimizing function\n",
        "num_features = len(FEATURES)\n",
        "model = BERTAndErnie(num_features)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKL5o6eEy9bj"
      },
      "outputs": [],
      "source": [
        "def validate():\n",
        "    \"\"\"\n",
        "    Evaluate model during trainging.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    fin_targets=[]\n",
        "    fin_outputs=[]\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _, data in enumerate(testing_loader, 0):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            features  = data['features'].to(device, dtype = torch.float)\n",
        "\n",
        "            outputs = model(ids, mask, features)\n",
        "\n",
        "            #compute argmax\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            _, labels = torch.max(targets, 1)\n",
        "\n",
        "            fin_targets.extend(labels.cpu().numpy().tolist())\n",
        "            fin_outputs.extend(preds.cpu().detach().numpy().tolist())\n",
        "\n",
        "    return fin_outputs, fin_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ydAfJ17y9bj",
        "outputId": "14ae634e-0cd4-479d-a535-90f9bd8db3fa"
      },
      "outputs": [],
      "source": [
        "# train the model\n",
        "saved_outputs = []\n",
        "saved_accuracy = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    model.train()\n",
        "    for idx, data in enumerate(training_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "        features = data['features'].to(device, dtype = torch.float)\n",
        "\n",
        "        # print('cat input', features.size())\n",
        "        outputs = model(ids, mask, features)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        if idx%100==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    preds, targets = validate()\n",
        "\n",
        "    # play with a softmax activation function in the classifier\n",
        "    accuracy = metrics.accuracy_score(targets, preds)\n",
        "    saved_accuracy.append(accuracy)\n",
        "    print(f\"Accuracy Score = {accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "85ea4b3f8f8bbee1e23da4c155a93a1e9f6833b5217ca791a452f768bdc9cb7b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
