{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torchdata\n",
    "import portalocker\n",
    "import time\n",
    "import pandas as pd\n",
    "import os, re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/claireboyd/courses/advanced_ml/dirty_comments_and_clean_plates'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"..\")\n",
    "os.path.abspath(os.curdir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing using Custom Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "    def __init__(self, df_filepath, ngram_range=None, clean_regex=None, max_features=None, stop_words=None):\n",
    "        # read in reviews to vectorizor object\n",
    "        self.df = pd.read_csv(df_filepath)\n",
    "        self.texts=self.df['reviews']\n",
    "\n",
    "        # vectorizor params\n",
    "        self.clean_regex = clean_regex\n",
    "        self.max_features = max_features #vocab size\n",
    "        self.stopwords = stop_words #if we want to remove these or not\n",
    "        self.ngram_range = ngram_range #size of ngrams to use as each observation\n",
    "        self.tfidf = TfidfVectorizer(analyzer='word',\n",
    "                                     stop_words=self.stopwords,\n",
    "                                     ngram_range=self.ngram_range, \n",
    "                                     max_features=self.max_features)\n",
    "\n",
    "    def clean_texts(self):\n",
    "        cleaned = []\n",
    "        for text in self.texts:\n",
    "            if self.clean_regex is not None:\n",
    "                text = re.sub(self.clean_regex,\" \",text)\n",
    "            text = text.lower().strip()\n",
    "            cleaned.append(text)\n",
    "        return cleaned\n",
    "    \n",
    "    def set_tfidf(self,cleaned_texts):\n",
    "        self.tfidf.fit(cleaned_texts)\n",
    "    \n",
    "    def build_vectorizer(self):\n",
    "        cleaned_texts = self.clean_texts()\n",
    "        self.set_tfidf(cleaned_texts)\n",
    "        \n",
    "    def vectorizeTexts(self):\n",
    "        cleaned_texts = self.clean_texts()\n",
    "        return self.tfidf.transform(cleaned_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2165, 7000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = Vectorizer(df_filepath=\"data/phila/labeled_inspections_with_reviews.csv\",\n",
    "                        clean_regex=\"[^a-zA-Z0-9]\",\n",
    "                        max_features=7000,\n",
    "                        ngram_range=(1,2), \n",
    "                        stop_words=\"english\")\n",
    "vectorizer.build_vectorizer()\n",
    "vectorized_x = vectorizer.vectorizeTexts().toarray()\n",
    "vectorized_x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, vectorized_reviews, df_filepath, y_col=\"y\", features=None):\n",
    "\n",
    "        # read in data and encode outcome variable\n",
    "        self.df = pd.read_csv(df_filepath)\n",
    "        self.df[[y_col]] = 0\n",
    "        self.df.loc[self.df.loc[:,'Overall Compliance'] == \"No\",y_col] = 1\n",
    "        \n",
    "        self.text = vectorized_reviews\n",
    "        self.labels = self.df.loc[:,'y']\n",
    "        self.features = features\n",
    "        \n",
    "        if self.features is not None:\n",
    "            self.features = self.df.loc[:,features]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        sample[\"text\"] = self.text[idx]\n",
    "        sample[\"labels\"] = self.labels[idx]\n",
    "        if self.features is not None:\n",
    "            sample[\"features\"] = self.features[idx] #.to_xarray()\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = ReviewsDataset(vectorized_reviews=vectorized_x,\n",
    "                              df_filepath=\"data/phila/labeled_inspections_with_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_freq = 0.8\n",
    "val_freq = 0.5 #of remaining % from train\n",
    "\n",
    "train_size = int(train_freq * len(full_dataset))\n",
    "val_test_size = len(full_dataset) - train_size\n",
    "train_dataset, val_test_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_test_size])\n",
    "\n",
    "val_size = int(val_freq * len(val_test_dataset))\n",
    "test_size = len(val_test_dataset) - val_size\n",
    "val_dataset, test_dataset = torch.utils.data.random_split(val_test_dataset, [val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap with any params here: https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(val_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([0])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([0])\n",
      "tensor([[0.0659, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n",
      "tensor([0])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "# see data in dataloader\n",
    "for i, data in enumerate(train_dataloader):\n",
    "    text = data['text']\n",
    "    labels = data['labels']\n",
    "    #features = data['features']\n",
    "    print(text)\n",
    "    print(labels)\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions needed across all model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_an_epoch(train_dataloader, optimizer, model, loss_function):\n",
    "    model.train() # Sets the module in training mode.\n",
    "    log_interval = 200\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        model.zero_grad()\n",
    "        log_probs = model(data['text'])\n",
    "        loss = loss_function(log_probs, data['labels'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print(f'At iteration {idx} the loss is {loss:.3f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(validation_dataloader, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ## WRITE YOUR CODE BELOW.    \n",
    "        n_correct = 0\n",
    "        n_examples = 0\n",
    "        for data in validation_dataloader:\n",
    "            # get predicted probabilities, and labels with highest probability\n",
    "            outputs = model(data['text'])\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "            \n",
    "            # count correct predictions & update counts\n",
    "            batch_correct = (predicted_labels == data['labels']).sum().item()\n",
    "            n_correct += batch_correct\n",
    "            n_examples += len(data['labels'])\n",
    "    \n",
    "    # calculate average accuracy across all batches\n",
    "    average_accuracy = n_correct / n_examples\n",
    "    return average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(train_dataloader, valid_dataloader, optimizer, model, loss_function, epochs):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=3)\n",
    "    accuracies=[]\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_an_epoch(train_dataloader, optimizer, model, loss_function)\n",
    "        accuracy = get_accuracy(valid_dataloader, model)\n",
    "        accuracies.append(accuracy)\n",
    "        time_taken = time.time() - epoch_start_time\n",
    "        print()\n",
    "        print(f'After epoch {epoch} the validation accuracy is {accuracy:.3f}.')\n",
    "        print()\n",
    "        \n",
    "    plt.plot(range(1, epochs+1), accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN INPUTS\n",
    "# model = XGBClassifier(nthread=4,\n",
    "#                       verbosity=1,\n",
    "#                       #n_estimators=2, \n",
    "#                       #max_depth=2, \n",
    "#                       learning_rate=0.3, #default \n",
    "#                       #lambda=1, #default\n",
    "#                       #objective='binary:logistic',\n",
    "#                       #objective='binary:logistic'\n",
    "#                      )\n",
    "\n",
    "# simple SVM\n",
    "# https://github.com/kazuto1011/svm-pytorch/blob/master/main.py\n",
    "model = torch.nn.Linear(7000, 2)\n",
    "loss_function = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=3)\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 200 the loss is -479.545.\n",
      "At iteration 400 the loss is -974.109.\n",
      "At iteration 600 the loss is -1477.942.\n",
      "At iteration 800 the loss is -1851.174.\n",
      "At iteration 1000 the loss is -2331.146.\n",
      "At iteration 1200 the loss is -946.508.\n",
      "At iteration 1400 the loss is -3370.203.\n",
      "At iteration 1600 the loss is -3825.008.\n"
     ]
    }
   ],
   "source": [
    "train_an_epoch(train_dataloader, optimizer, model, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4161.7091, 1206.6766]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': tensor([[0., 0., 0.,  ..., 0., 0., 0.]]), 'labels': tensor([0])}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#why is my accuracy staying constant?\n",
    "#plot_accuracy(train_dataloader, valid_dataloader, optimizer, model, loss_function, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
